# Feature Specification: AI-Powered Todo Chatbot Integration for Hackathon II Phase III

**Feature Branch**: `2-todo-ai-chatbot`
**Created**: 2026-01-13
**Status**: Draft
**Input**: User description: "AI-Powered Todo Chatbot Integration - Hackathon II Phase 3

## Target audience:
Hackathon judges evaluating integration of AI conversational interface with Phase II app, stateless architecture, MCP tools usage, natural language processing for Basic features, and TDD compliance; Panaversity team assessing AI agent development skills and Phase III readiness for cloud-native evolution

Focus: Integrate a stateless AI-powered chatbot into the Phase II full-stack Todo web app using OpenAI ChatKit for frontend UI, FastAPI chat endpoint, OpenAI Agents SDK for natural language processing and tool calling, OpenRouter free model as LLM, Official MCP SDK for exposing task operations as tools, with persistent conversation state in Neon DB; enable logged-in users to manage Basic Level Todo features conversationally while providing user-specific task summaries (total tasks, completed/pending counts, priority breakdowns)

## Success criteria:
- Implements conversational interface for all Basic Level features: add task, remove (delete) task, update task, toggle mark complete/incomplete, show task list via natural language commands
- Chatbot requires user login and accesses only authenticated user's tasks; provides accurate summaries including total tasks, completed count, pending count, and priorities (low/medium/high counts)
- Builds stateless chat endpoint that persists full conversation history (Conversation and Message models) to Neon DB, allowing seamless resumption across sessions
- Defines and implements MCP tools (add_task, list_tasks, complete_task, delete_task, update_task) with precise parameters, returns, and examples; agent chains tools as needed for complex queries
- Agent behavior handles natural language variations with confirmations, error grace, and summaries; e.g., "What's pending?" triggers list_tasks with status="pending" and aggregates counts/priorities, Agent can use English (LTR), Urdu (RTL) and Roman Urdu language (LTR) to communicate
- Achieves TDD coverage: 80%+ backend (100% for MCP tools/auth), 70%+ frontend, 100% E2E for conversational flows (login → chat "add task" → confirm → summary)
- Demonstrates reusable intelligence via Phase III-specific subagents/skills for MCP, agent dev, chat endpoint; optional Urdu support in chatbot responses and voice input for commands
- Produces working chatbot deployable with Phase II app,

## Constraints:
- No manual coding: All Phase III code generated by Claude Code from refined specs; integrate without altering Phase II code/structure
- Strict TDD: Red-Green-Refactor for all new components (chat endpoint, MCP server, agents, DB models); no production code without failing test
- Technology stack: Frontend (OpenAI ChatKit with domain allowlist); Backend (FastAPI stateless endpoint, OpenAI Agents SDK, OpenRouter free Model for LLM, Official MCP SDK); ORM (SQLModel for Conversation/Message models); Database (extend Neon PostgreSQL from Phase II); Auth (reuse Better Auth JWT from Phase II)
- Monorepo extension: Add Phase III specs to /specs/ (features/chatbot.md, api/mcp-tools.md, database/conversation-schema.md); maintain history/prompts/ for PHRs
- Stateless architecture: No server-held state; persist all (conversations, messages, tool calls) to DB; MCP tools stateless with DB ops
- Timeline: Complete by Dec 21, 2025 checkpoint; submission via form with GitHub repo, Vercel link, <90s demo video

## Not building:
- Intermediate/Advanced Todo features in chatbot (priorities, search/filter/sort, recurring, reminders—focus Basic only)
- Phase IV/V elements (Kubernetes, Helm, Kafka, Dapr, AIOps)
- Custom AI models or training (use OpenAI Agents SDK as-is it)
- Ethical AI discussions or bias handling in chatbot
- Mobile/native integrations or non-web chatbot UI


Below is the example Code to use OpenAI SDK with OpenRouter:


```python
import os
from dotenv import load_dotenv
from agents import (Agent, Runner, OpenAIChatCompletionsModel, set_tracing_disabled, ModelSettings, function_tool)
from openai import AsyncOpenAI

load_dotenv()
set_tracing_disabled(True)

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
if not OPENROUTER_API_KEY:
    raise Exception("Missing OPENROUTER_API_KEY environment variable")

OPENROUTER_URL = os.getenv("OPENROUTER_URL")
if not OPENROUTER_URL:
    raise Exception("Missing OPENROUTER_URL environment variable")

# Initialize the AsyncOpenAI-compatible client with OpenRouter details
external_client: AsyncOpenAI = AsyncOpenAI(
    api_key=OPENROUTER_API_KEY,
    base_url=OPENROUTER_URL,
)


# Model Initialization - using OpenRouter model with OpenAI-compatible interface
model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(
    model="mistralai/devstral-2512:free",  # Using OpenRouter free model
    openai_client=external_client
)


# Create agent and register tools
agent = Agent(
    name="Physical AI & Humanoid Robotics Tutor",  # Agent's identity
    instructions=(
        """
    - You are an expert AI tutor for the Physical AI & Humanoid Robotics textbook.
    - When a user asks a question, always first call the `retrieve` tool with the user query,
    then use only the return content from retrieve to answer.
    - If the answer is not in the retrieved content via tool, say `I'm sorry. I am here to help you about Physical AI & Humanoid-robotics textbook, so I can only entertain you with information from the textbook.`
    - For section-selected queries where a user wants to know about specific text, use the same retrieval process but focus your response on the selected text.
        * When user highlight task, a dialog box or dropdown menu will appear having option to `Ask AI`, when user click on it, it will directly go to Chatbot as a query and agent will response accordingly.
    - You can add addition information from your own knowledge base to your response but it will be strictly relevant to the book topic and user query about Physical AI & Humanoid-robotics.
    - User ask via `chatbot query` or ask via `selected text`, you will answer within 80 to 150 words.
    """
    ),
    model=model,
    #tools=[retrieve],  If you used any tool then we will add tool like this
    model_settings=ModelSettings(
        tool_choice="required"
    )
)

# For sync method of Runner, it will run on CLI my below code

result = Runner.run_sync(
     starting_agent=agent,
     input="What is Humanoid Robotics?",
)

```

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Chatbot Authentication and Task Summary (Priority: P1)

A logged-in user accesses the chatbot interface and receives an initial task summary, then can interact with their tasks using natural language. This establishes the foundational chat experience that enables all other functionality.

**Why this priority**: Without authentication and user isolation, the entire multi-user system cannot function. The chatbot must integrate with the existing Phase 2 security model and provide immediate value with task summaries.

**Independent Test**: Can be fully tested by authenticating to the chatbot, receiving a task summary, and verifying user isolation. Delivers secure multi-user isolation with JWT tokens and initial task overview.

**Acceptance Scenarios**:

1. **Given** a user is logged into the Phase 2 app, **When** they access the chatbot interface, **Then** they receive a task summary showing total tasks, completed tasks, pending tasks, and priority breakdown (low/medium/high counts)
2. **Given** a user has valid JWT token, **When** they start a new chat conversation, **Then** the chatbot is authenticated as that user and can only access their tasks
3. **Given** a user with no tasks, **When** they access the chatbot, **Then** they receive a welcome message and can create tasks

---

### User Story 2 - Natural Language Task Management (Priority: P1)

A user can create, view, update, and delete tasks using natural language commands instead of UI forms. This is the core functionality of the AI chatbot.

**Why this priority**: This represents the fundamental purpose of the chatbot - enabling natural language task management. Without this, the chatbot has no value.

**Independent Test**: Can be fully tested by using natural language commands to manage tasks. Delivers core conversational task management functionality.

**Acceptance Scenarios**:

1. **Given** a user is in the chat interface, **When** they say "Add a task to buy groceries", **Then** a new task "Buy groceries" is created and added to their task list
2. **Given** a user has tasks in their list, **When** they say "Show me all my tasks" or "What's on my list?", **Then** they receive a list of their tasks
3. **Given** a user has tasks in their list, **When** they say "Mark task 'Buy groceries' as complete" or "Complete the meeting task", **Then** the specified task is marked as completed
4. **Given** a user has tasks in their list, **When** they say "Delete the shopping task", **Then** the specified task is removed from their list

---

### User Story 3 - Advanced Chat Operations (Priority: P2)

A user can perform more complex operations like filtering tasks, getting summaries, and multi-step operations through natural language.

**Why this priority**: These features significantly enhance the user experience, making the chatbot more valuable than simple UI interactions.

**Independent Test**: Can be fully tested by using complex natural language commands. Delivers enhanced conversational task management capabilities.

**Acceptance Scenarios**:

1. **Given** a user has multiple tasks, **When** they say "Show me my high priority tasks", **Then** only high priority tasks are displayed
2. **Given** a user has completed tasks, **When** they say "What have I completed today?", **Then** they receive a list of recently completed tasks
3. **Given** a user wants to update multiple aspects of a task, **When** they say "Change the meeting task to 'Team sync call' and make it high priority", **Then** both title and priority are updated

---

### User Story 4 - Conversation Context and History (Priority: P2)

A user can maintain conversation context across multiple messages and resume conversations with preserved history.

**Why this priority**: Context preservation is essential for natural, flowing conversations and prevents users from having to repeat information.

**Independent Test**: Can be fully tested by starting conversations, having multiple exchanges, and resuming conversations. Delivers persistent conversation state management.

**Acceptance Scenarios**:

1. **Given** a user has an ongoing conversation, **When** they ask "What did I just add?", **Then** the chatbot references the previous message in its response
2. **Given** a user has a previous conversation, **When** they resume chatting, **Then** they can continue from where they left off with preserved context
3. **Given** a user is in a conversation about a specific task, **When** they say "update it", **Then** the chatbot understands they're referring to the previously mentioned task

---

### User Story 5 - Multilingual Support (Priority: P3 - Bonus Feature)

A user can interact with the chatbot in both English and Urdu languages, with the chatbot responding appropriately in the same language.

**Why this priority**: This enhances accessibility for users who prefer to interact in Urdu, making the application more inclusive.

**Independent Test**: Can be fully tested by sending messages in both English and Urdu and verifying appropriate responses. Delivers multilingual task management capabilities.

**Acceptance Scenarios**:

1. **Given** a user sends a task command in Urdu, **When** they say "کام شامل کریں گھر سفید کرنا", **Then** a new task "گھر سفید کرنا" is created and added to their task list
2. **Given** a user sends a query in Roman Urdu, **When** they say "Show kro sab tasks", **Then** they receive a list of their tasks
3. **Given** a user interacts in Urdu, **When** they ask for task completion, **Then** the chatbot responds in Urdu with appropriate confirmation

---

### Edge Cases

- What happens when a user tries to access another user's tasks through the chatbot? (System must enforce user isolation and return appropriate error)
- How does the system handle expired JWT tokens in chat requests? (System must return 401 and prevent chat operations)
- What happens when the AI misinterprets natural language commands? (System must provide helpful clarification or error messages)
- How does the system handle network failures during MCP tool execution? (System must show appropriate error messages and allow retry)
- What happens when a user provides ambiguous task references? (System must ask for clarification)
- How does the system handle very long conversation histories? (System must implement token management and history truncation)
- What happens when a user sends messages in mixed languages? (System should detect dominant language and respond appropriately)
- How does the system handle rate limiting for frequent chat requests? (System must return 429 and implement backoff strategy)

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: System MUST authenticate chat requests via JWT tokens with user_id extraction and validation
- **FR-002**: System MUST route user messages to OpenAI Agent SDK with MCP tools for task operations
- **FR-003**: System MUST enforce strict multi-user isolation - users can only access their own tasks via chat
- **FR-004**: System MUST support natural language task creation (e.g., "Add a task to buy groceries", "کام شامل کریں گھر سفید کرنا", "Add task buy milk")
- **FR-005**: System MUST support natural language task listing (e.g., "Show me my tasks", "What's pending?", "Meray tasks dikhao")
- **FR-006**: System MUST support natural language task completion (e.g., "Mark task X as complete", "Complete the meeting task", "Task complete krdo")
- **FR-007**: System MUST support natural language task deletion (e.g., "Delete task X", "Remove the shopping task", "Task delete krdo")
- **FR-008**: System MUST support natural language task updates (e.g., "Change task X to Y", "Update meeting to 'Team sync'", "Task update kro")
- **FR-009**: System MUST store conversation history in Neon PostgreSQL with user_id scoping
- **FR-010**: System MUST retrieve conversation history for context in multi-turn conversations
- **FR-011**: System MUST implement MCP tools (add_task, list_tasks, complete_task, delete_task, update_task, get_task_summary) with user_id validation
- **FR-012**: System MUST provide natural language responses to user commands with friendly confirmation messages
- **FR-013**: System MUST handle ambiguous requests by asking for clarification when needed
- **FR-014**: System MUST maintain conversation state across multiple requests (stateless server with database persistence)
- **FR-015**: System MUST implement rate limiting for chat requests (100 requests per minute per user)
- **FR-016**: System MUST provide 60fps animations and responsive UI matching Phase 2 design system
- **FR-017**: System MUST support multilingual commands (English + Urdu as Phase 3 bonus feature)
- **FR-018**: System MUST handle MCP tool execution errors gracefully and return appropriate messages to users
- **FR-019**: System MUST implement proper timeout handling for agent execution (default 30 seconds)
- **FR-020**: System MUST validate all MCP tool parameters and return meaningful error messages
- **FR-021**: System MUST provide task summary on user login (total tasks, completed count, pending count, priority distribution)
- **FR-022**: System MUST handle recurring task creation through natural language (when recurrence pattern is specified)
- **FR-023**: System MUST support voice input processing for task commands (Phase 3 bonus feature)
- **FR-024**: System MUST implement smart suggestions for task priorities, due dates, and improvements based on context

### Non-Functional Requirements

- **NFR-001**: Backend MUST achieve 80%+ test coverage overall with 100% coverage for authentication and security
- **NFR-002**: Agent behavior tests MUST achieve 80%+ coverage for natural language understanding
- **NFR-003**: MCP tool tests MUST achieve 90%+ coverage for all core operations
- **NFR-004**: System MUST respond to chat requests within 3 seconds for 95th percentile of requests
- **NFR-005**: MCP tools MUST execute with <100ms latency and proper user isolation
- **NFR-006**: System MUST handle 1000+ concurrent users without performance degradation
- **NFR-007**: Natural language understanding MUST work for both English and Urdu (Phase 3 bonus feature)
- **NFR-008**: System MUST maintain 99.9% uptime during normal operation
- **NFR-009**: System MUST provide WCAG 2.1 AA accessibility compliance
- **NFR-010**: Database operations MUST use parameterized queries to prevent SQL injection
- **NFR-011**: System MUST implement rate limiting of 100 requests per minute per user with appropriate 429 responses and Retry-After headers

### Key Entities

- **User**: Represents an authenticated user with unique identifier from JWT token, same as Phase 2
- **Task**: Represents a todo item managed through MCP tools, same as Phase 2 database schema
- **Conversation**: Represents a chat session with unique identifier, user ownership, and metadata
- **Message**: Represents individual chat messages (user input or AI response) with role, content, and tool execution context
- **MCP Tool**: Represents stateless functions (add_task, list_tasks, etc.) that operate on user's tasks with proper authentication

## Clarifications

### Session 2026-01-13

- Q: What specific natural language patterns should the chatbot recognize for task creation? → A: Support multiple phrasings like "Add task to X", "Create task X", "I need to X", "Remember to X", "Add a task called X to do Y", "کام شامل کریں X", "X کے لئے کام بنائیں", etc. The agent should extract task titles and descriptions from various sentence structures in both English and Urdu.
- Q: How should the system handle task identification when users reference tasks by name? → A: Use list_tasks to find tasks by partial name matching, then execute operations on the matched task. If multiple matches exist, ask for clarification. Support both English and Urdu task names.
- Q: What is the expected response format from MCP tools to the AI agent? → A: MCP tools return structured JSON with operation status, task details, and timestamps. The agent uses this information to form natural language responses in the user's language.
- Q: How should conversation history be managed for context? → A: Store conversation history in database with chronological ordering. Retrieve recent messages for context when building message arrays for agent execution. Implement token management to prevent exceeding API limits.
- Q: What happens when the AI fails to understand a command? → A: The system should return helpful, friendly messages asking for clarification or suggesting alternative commands. Never expose technical errors to the user.
- Q: What frontend approach should be used for the chatbot UI? → A: Use OpenAI ChatKit (hosted solution) with domain allowlist configuration.
- Q: How should the MCP server be implemented and integrated? → A: Build MCP server with Official MCP SDK that exposes task operations as tools to agent. AI agents use MCP tools to manage tasks. The MCP tools will also be stateless and will store state in the database. Official MCP SDK will be used to make MCP server. The MCP server must expose the tools for the AI agent.
- Q: How should OpenAI Agent be integrated with the MCP server? → A: The FastAPI backend handles the stateless chat endpoint and uses OpenAI Agents SDK for orchestration (agent logic and runner), while connecting to a separate MCP server (built with Official MCP SDK) that exposes stateless tools for task operations. This separation ensures modularity, with all state (tasks, conversations, messages) persisted in the Neon DB via SQLModel. It aligns perfectly with the spec for natural language todo management.
- Q: How should conversation persistence be handled? → A: Conversation and message persistence is handled through a stateless architecture using the Neon Serverless PostgreSQL database via SQLModel. Key elements include:

Database Models:
Conversation: Stores session metadata (user_id, id, created_at, updated_at) to group related messages.
Message: Stores individual chat entries (user_id, id, conversation_id, role [user/assistant], content, created_at) for history.

Stateless Request Cycle (for the /api/{user_id}/chat endpoint):
Receive user message (with optional conversation_id; create new if none provided).
Fetch full conversation history from the database.
Build the message array (history + new user message) for the OpenAI Agents SDK.
Store the user message in the database.
Run the agent, which may invoke MCP tools (also stateless, persisting any changes to tasks in the DB).
Store the assistant's response in the database.
Return the response (including conversation_id and any tool calls) to the client.
No state is held on the server post-request.

This ensures conversations can resume seamlessly (e.g., after server restarts) by reloading history from the DB, maintaining context for natural language todo management while keeping the FastAPI backend and MCP server modular and scalable.
- Q: How should security and authentication be implemented? → A: Validate JWT token on each request, verify user_id matches token claims.
- Q: What is the complete request flow from user message to response? → A: User sends message → FastAPI validates JWT → loads conversation history → sends to OpenAI Agent SDK → agent calls MCP tools → tools update DB → agent generates response → FastAPI saves response to DB → returns to user. This matches the hackathon's architecture emphasis:

FastAPI backend as the primary entry point (handling the /api/{user_id}/chat endpoint)
JWT validation occurs in FastAPI middleware (consistent with Phase II's JWT setup for REST API security, extended to the chat endpoint)
Conversation history is loaded from Neon DB (via SQLModel) by FastAPI
OpenAI Agents SDK is used in the backend to orchestrate the agent run (building message array from history + new user input)
The agent invokes tools, which are exposed via a separate MCP server (Official MCP SDK) for stateless tool execution
Tools persist changes directly to the DB (e.g., create/update/delete tasks)
FastAPI saves the assistant response (and any new messages) to the DB after the agent completes
Response is returned to the frontend/user (including conversation_id for continuity)
- Q: How are conversation/message data and task data kept synchronized? → A: Conversation/message data and task data are kept synchronized through the database (Neon Serverless PostgreSQL) — there is no separate in-memory or agent-held state.

Conversation persistence
→ Stored in dedicated Conversation and Message tables (via SQLModel).
→ Every user message and every assistant response is explicitly saved to the DB during the request cycle.
Task operations
→ Performed by stateless MCP tools (exposed via Official MCP SDK).
→ When the agent (orchestrated by OpenAI Agents SDK in FastAPI) decides to call a tool (add/update/delete/complete task, etc.), the tool directly reads from / writes to the same Neon DB using SQLModel.
Synchronization mechanism
→ Both layers (chat history + tasks) share the same PostgreSQL database.
→ Any task change made via an MCP tool is immediately persisted in the DB → the next time the conversation loads history (in a future request), the latest task state is visible if the agent needs to reference or operate on tasks again.
→ No explicit "sync" step is needed — database writes ensure eventual consistency (and immediate consistency within a single request).

This design keeps the system stateless at the service level (FastAPI + MCP server) while relying on the shared persistent Neon DB as the single source of truth for both chat context and todo items.
- Q: What LLM model should be used for the agent? → A: We will use OpenRouter but model will be confirmed later, for now you can go with mistralai/devstral-2512:free.
- Q: Should Phase 3 maintain backward compatibility with Phase 2? → A: All existing Phase 2 API endpoints and functionality remain unchanged, with Phase 3 adding only new chat capabilities.
- Q: How should MCP tools access existing Phase 2 task data? → A: In Phase III (per the hackathon docs), the MCP tools (built with Official MCP SDK) handle access to existing Phase II task data as follows:

MCP server exposes stateless tools for todo operations (e.g., list_tasks, get_task, create_task, update_task, delete_task, complete_task).
Each tool directly queries / modifies the same Neon Serverless PostgreSQL database (shared with FastAPI backend) using SQLModel.
Tools enforce user isolation by requiring user_id (passed via tool arguments or auth context from the agent call) and filtering all DB operations to tasks where task.user_id == authenticated user_id.
This mirrors Phase II's JWT-filtered REST API logic but via MCP tool schema — no separate auth layer in MCP; FastAPI already validates JWT and passes user context if needed, but tools themselves query/filter by user_id to prevent cross-user access.
Synchronization is automatic: tool writes (e.g., add/update task) persist immediately to DB → next conversation load (or future tool call) sees the latest state.

Short answer: Direct DB access via SQLModel, filtered by user_id for security and consistency with Phase II.

### Session 2026-01-13 (continued)

- Q: How should the chatbot handle multiple tasks with similar names? → A: When ambiguity exists, the agent should list possible matches and ask the user to specify which task they mean, or provide additional context like "the meeting task from yesterday" vs "the meeting task for tomorrow". Support this in both English and Urdu.
- Q: What is the API endpoint pattern for chat operations? → A: All chat endpoints follow the /api/{user_id}/chat pattern to ensure user isolation with JWT token validation, consistent with Phase 2 patterns.
- Q: How should the system handle rate limiting for MCP tool calls? → A: Implement per-user rate limits for each tool type (add_task: 100/min, list_tasks: 500/min, complete_task: 100/min, delete_task: 50/min, update_task: 100/min) with appropriate error responses.
- Q: What authentication context is available to MCP tools? → A: Each MCP tool receives user_id as a parameter from the authenticated JWT token, ensuring all operations are scoped to the authenticated user.
- Q: How should error recovery work when MCP tools fail? → A: Tools should return structured error responses that the agent can interpret and convert to user-friendly messages. The system should handle partial successes gracefully.
- Q: What is the detailed specification for the get_task_summary MCP tool? → A: The get_task_summary tool retrieves task summary statistics for the authenticated user. Parameters: user_id (string, required). Returns: Object with summary statistics including total_tasks (integer), completed_tasks (integer), pending_tasks (integer), and priority_breakdown (object with high/medium/low counts). Example Input: {"user_id": "ziakhan"}. Example Output: {"total_tasks": 10, "completed_tasks": 6, "pending_tasks": 4, "priority_breakdown": {"high": 2, "medium": 1, "low": 1}}.

### Session 2026-01-13 (Phase 3 Bonus Features)

- Q: How should Urdu language support be implemented? → A: The natural language parsing skill should support intent recognition in both English and Urdu with appropriate response templates in both languages. This enables the Phase 3 bonus multilingual feature.
- Q: What are the requirements for conversation persistence? → A: All conversation history must be stored in the database with user_id scoping. Conversations should persist across server restarts and be accessible when users resume chatting.
- Q: How should the system handle concurrent requests from the same user? → A: Implement proper serialization or optimistic locking to prevent race conditions when multiple messages arrive simultaneously.
- Q: How should voice input be processed for task commands? → A: Implement speech-to-text processing that converts voice commands to text before natural language processing. Support both English and Roman Urdu voice commands.
- Q: What kind of smart suggestions should the AI provide? → A: AI should suggest task improvements, priorities, due dates, and categories based on context, user history, and common patterns. For example, suggesting "HIGH" priority for "urgent" tasks or "WEEKLY" recurrence for "weekly meeting" tasks.

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: Users can successfully create tasks via natural language commands with 90%+ accuracy
- **SC-002**: The system enforces 100% user isolation - no user can access another user's tasks via chat
- **SC-003**: Chatbot responds to user messages within 3 seconds for 95% of requests
- **SC-004**: 95% of users successfully complete primary task management workflows via chat on first attempt
- **SC-005**: The application achieves 80%+ backend test coverage and 80%+ agent behavior test coverage
- **SC-006**: All authentication and security features achieve 100% test coverage
- **SC-007**: Critical chat flows (create, list, complete, delete) achieve 100% E2E test coverage
- **SC-008**: MCP tools execute with <100ms latency and proper user isolation
- **SC-009**: The chatbot UI responds with 60fps animations and matches Phase 2 design system
- **SC-010**: Natural language understanding works for both English and Urdu (Phase 3 bonus)
- **SC-011**: Task summary provides accurate counts of total, completed, pending tasks and priority distribution
- **SC-012**: Conversation history persists across sessions with preserved context
- **SC-013**: System handles 100+ concurrent chat sessions without degradation
- **SC-014**: Rate limiting prevents abuse while allowing legitimate usage patterns
- **SC-015**: Voice input processing works for English and Roman Urdu commands (Phase 3 bonus)